---
title: "02_corpus_preproc"
format: html
editor: visual
---

# Corpus preprocessing

This notebook prepare data for its use with BDI package. Namely, it is used to:

-   chunk *problems* into 1000-words chunks

-   filter reference set to balance over-represented texts by major authors

-   look into the words distribution in the corpus (in case there is some errors, e.g., as "et" left as "&" and not presented in the words frequencies

```{r}
library(tidyverse)
library(tidytext)

theme_set(theme_minimal())
```

## Ground truth set selection

Select Diderot II as an unknown author

```{r}
corpus <- readRDS("../data/corpus_chunks.rds")

glimpse(corpus)
```

Reshape columns and add numerical author's labels

```{r}
corpus <- corpus %>% 
  mutate(tag = chunk_id) %>% # separate columns into work, etc.
  separate(chunk_id, into = c("chunk_num", "title"), sep = "__") %>% 
  separate(title, into = c("author", "work"), sep = "_") 

# author labels
lbls <- tibble(
  author = unique(corpus$author)
) %>% mutate(author_label = row_number())

corpus <- corpus %>% 
  left_join(lbls, by = "author") %>% 
  
  # reorder columns
  select(work, author_label, author, chunk_num, tag, text) 
```

Select Diderot texts as problems

```{r}
diderot_quest <- corpus %>% 
  filter(author == "Diderot II") %>% 
  # filter out two works with 13 and 12 chunks respectively
  filter(work %in% c("Pensees philosophiques", "Salon 1781")) %>% 
  
  # downsize to 10 chunks
  filter(as.numeric(chunk_num) < 10)

diderot_quest %>% 
  count(author, work)
```

Remove all other Diderot II, reduce the corpus size overall

```{r}
# mean & median number of chunks in works in the coprus
corpus %>% 
  filter(author != "Diderot II") %>% 
  count(author, work, sort = T) %>% 
  summarise(mn = mean(n),
            md = median(n))

# filter works with more than 44 chunks
long_works <- corpus %>% 
  filter(author != "Diderot II") %>% 
  count(author, work, sort = T) %>% 
  filter(n > 44)

# sample only 30 chunks from these
long_works_s <- corpus %>% 
  filter(author != "Diderot II") %>% 
  filter(work %in% long_works$work) %>% 
  group_by(work) %>% 
  sample_n(30)

long_works_s %>% count(author, work)

# merge with the rest
corpus_s <- corpus %>% 
  filter(author != "Diderot II") %>% 
  filter(!work %in% long_works$work) %>% 
  rbind(long_works_s) %>% 
  
  # attach Diderot II as well
  rbind(diderot_quest)

# check number of chunks for each author
corpus_s %>% count(author, sort = T)

rm(lbls, long_works, long_works_s)
```

Count & scale words

```{r}
glimpse(corpus_s)

# total words
total <- corpus_s %>% 
  unnest_tokens(input = text, output = word, token = "words") %>% 
  nrow()

# check ranks for the 200 MFW
ranks <- corpus_s %>% 
  unnest_tokens(input = text, output = word, token = "words") %>% 
  count(word, sort = T) %>% 
  mutate(rel_freq = (n / total) * 100 ) %>% 
  head(200)

head(ranks, 100)
ranks$word
```

Authorial word freqs

```{r}
# number of unique authors
unique(corpus_s$author) # 19 with Diderot II

# total number of words by each author
total_author <- corpus_s %>% 
  unnest_tokens(input = text, output = word, token = "words") %>% 
  count(author, sort = T) %>% 
  mutate(perc = n / total * 100)
  
total_author

authors_freqs <- corpus_s %>% 
  unnest_tokens(input = text, output = word, token = "words") %>% 
  group_by(author) %>% 
  count(word, sort = T) %>% 
  ungroup() %>% 
  left_join(total_author %>% 
              select(-perc) %>% 
              rename(total = n), 
            by = "author") %>% 
  mutate(rel_freq = n / total * 100) %>% 
  
  # filter only 200 MFW (as in total)
  filter(word %in% ranks$word) %>% 
  
  group_by(author) %>% 
  mutate(author_rank = row_number()) %>% 
  ungroup()

head(authors_freqs)
authors_freqs %>% count(word, sort = T)
```

Some background check on 200MFW freqs by different authors

```{r}
unique(authors_freqs$author)

authors_freqs %>% 
  #filter(author %in% c("Diderot", "dHolbach", "Deleyre", "Pechmeja", "Raynal")) %>% 
  filter(author_rank < 50) %>% 
  
  left_join(ranks %>% rename(total_rel_freq = rel_freq) %>% select(-n),
            by = "word") %>% 
  mutate(total = "total") %>% 
  
  ggplot(aes(x = reorder_within(word, by = -rel_freq, within = word), y = rel_freq, color = author, group = author)) + 
  geom_line() + 
  scale_x_reordered() + 
  theme(axis.text.x = element_text(angle = 90)) + 
  geom_line(aes(y = total_rel_freq, group = total), color = "black", lty = 2)
```

Create a shorter sample (less authors) & scale word freqs

```{r}
corpus_s %>% 
  filter(author %in% c("Diderot", "Deleyre", "dHolbach", "Jussieu", "Raynal", 
                       "Diderot II")) %>% 
  unnest_tokens(input = text, output = word, token = "words") %>% 
  filter(word %in% ranks$word) %>% 
  group_by(tag) %>% 
  count(word, sort = T) %>% 
  ungroup() %>% 
  pivot_wider(names_from = word, values_from = n, values_fill = 0)
```

## Problems

Load data for texts to be attributed

```{r}
editions <- list.files(path = "../data/test_fragments/",
                       pattern = "ed",
                       full.names = T)

editions
```

```{r}
pencil_ink <- list.files(path = "../data/test_fragments/",
                         pattern = "ink|pencil",
                         full.names = T)

p_ink <- tibble(
  path = pencil_ink,
  title = c("HDI_ink", "HDI_pencil"),
  text = sapply(path, read_file)
)

str(p_ink)
```

```{r}
# clean ' and search for &
# remove "appeared|changed|unchanged"

p_ink_tokens <- p_ink %>% 
  # replace ' and - with \s
  mutate(text_cln = str_replace_all(text, "'|-|_", " "),
         # replace newlines
         text_cln = str_replace_all(text_cln, "\\r\\n", "  "),
         # lowercase
         text_cln = sapply(text_cln, tolower)) %>% #str
  # remove raw texts & paths
  select(-text, -path) %>% 
  rename(text = text_cln) %>% 
  
  # tokenisation
  unnest_tokens(input = text, output = word, token = "words") %>% 
  
  # remove working tags
  filter(!word %in% c("appeared", "changed", "unchanged")) %>% 
  # remove digits
  filter(!str_detect(word, "^\\d*$")) 

head(p_ink_tokens)
```

Create 1000-words chunks

```{r}
p_ink_chunks <- p_ink_tokens %>% 
  group_by(title) %>% 
  mutate(id = row_number()-1,
         id_group = floor(id /1000)) %>% 
  ungroup()

head(p_ink_chunks)

# remove chunks with less than 1000 words
x <- p_ink_chunks %>% 
  group_by(title, id_group) %>% 
  count() %>% 
  ungroup() %>% filter(n < 1000) %>% 
  mutate(chunk_id = paste0(title, "__", id_group)) 

x

p_ink_chunks <- p_ink_chunks %>% 
  mutate(chunk_id = paste0(title, "__", id_group)) %>% 
  filter(!chunk_id %in% x$chunk_id) %>% 
  
  # concat text in each chunk
  select(chunk_id, word) %>% 
  group_by(chunk_id) %>% 
  mutate(text = paste(word, collapse = " ")) %>%
  select(-word) %>%
  distinct() %>%
  ungroup()

head(p_ink_chunks)

# count number of chunks
p_ink_chunks %>% 
  separate(col = chunk_id, into = c("title", "chunk_id"), sep = "__") %>% 
  count(title)

rm(p_ink, p_ink_tokens)
```

Check text

```{r}
p_ink_chunks$text[3]
```

Create columns for BDI

```{r}
glimpse(p_ink_chunks)
```

```{r}
p_ink_chunks <- p_ink_chunks %>% 
  separate(col = chunk_id, into = c("title", "chunk_num"), sep = "__") %>% 
  separate(col = title, into = c("author", "work"), sep = "_") 

head(p_ink_chunks)
```

## Comparison set

```{r}
corpus <- readRDS("../data/corpus_chunks.rds")

glimpse(corpus)

# check random chunk
corpus$text[sample(nrow(corpus), 1)]
```

```{r}
raw_corpus <- corpus %>% 
  separate(chunk_id, into = c("chunk_num", "title"), sep = "__") %>% 
  separate(title, into = c("author", "work"), sep = "_") %>% 
  select(author, work, chunk_num, text) %>% 
  rbind(p_ink_chunks) 

head(raw_corpus)
```

Select only 5 authors & 3 works for each

```{r}
unique(raw_corpus$author)

short_corpus <- raw_corpus %>% 
  filter(author %in% c("Diderot", "Deleyre", "dHolbach", "Naigeon", "Raynal", 
                       "HDI")) %>% 
  count(author, work, sort = T) %>% 
  group_by(author) %>% 
  slice_max(n = 3, order_by = n) %>% 
  ungroup() %>% 
  mutate(tag = paste0(author, "_", work))

x <- raw_corpus %>% 
  mutate(tag = paste0(author, "_", work)) %>% 
  filter(tag %in% short_corpus$tag)

others <- x %>% 
  group_by(tag) %>% 
  sample_n(17) %>% 
  ungroup()  %>% 
  filter(author != "HDI")

# shorten hdi samples to 10
# leave the else to 17

# calculate freqs & normalise with L2
```

NB Pechmeja & Meister have 87 and 27 chunks, the rest \>160
