---
title: "03_ground-truth"
format: html
editor: visual
---

## Ground truth testing

Prepare data for the assessing how good the works of known authors in the corpus are attributed to their authors.

This notebook shows the preprocessing and visualisation of results obtained from python notebook with the same name.

```{r}
library(tidyverse)
library(tidytext)

library(MetBrewer)
library(cowplot)
theme_set(theme_minimal())
```

## Authors to themselves

This part of the notebook select 10 works by each author and use them to see how well BDI attribute the works to their authosr.

### 2000 words chunks

```{r}
corpus <- readRDS("../../data/bdi/corpus_chunks.rds")

glimpse(corpus)
```

Reorganize to 2000-words chunks

```{r}
corpus <- corpus %>% 
  separate(chunk_id, into = c("chunk_num", "title"), sep = "__") %>% 
  separate(title, into = c("author", "work"), sep = "_") %>% 
  select(author, work, chunk_num, text) %>% 
  # fix Diderot into one set
  mutate(author = ifelse(author == "Diderot II", "Diderot", author)) %>% 
  
  # recalculate chunks
  mutate(chunk_num_2 = floor(as.numeric(chunk_num)/2)) %>% 
  mutate(tag = paste0(chunk_num_2, "__", author, "_", work)) 

# last 1k chunks (to be removed, 79 chunks)
x <- corpus %>% 
  group_by(tag) %>% 
  count(chunk_num_2, sort = T) %>% 
  filter(n < 2) %>% 
  ungroup() %>% 
  pull(tag)

head(x)

corpus <- corpus %>% 
  # remove chunks with less then 2k words
  select(-chunk_num_2) %>% 
  filter(!tag %in% x) %>% 
  mutate(chunk_num = str_extract(tag, "^\\d+"))
  
head(corpus) # in this corpus each 2k chunk = 2 rows
```

Number of works by author

```{r}
corpus %>% 
  count(author, work, sort = T) %>% 
  group_by(author) %>% 
  count(sort = T)
```

Select 8 random works from more presented authors

```{r}
a <- corpus %>% 
  count(author, work, sort = T) %>% 
  group_by(author) %>% 
  count(sort = T) %>% 
  filter(n >= 10) %>% pull(author)

w <- corpus %>% 
  filter(author %in% a) %>% 
  count(author, work, sort = T) %>% 
  filter(n < 75) %>% 
  group_by(author) %>% 
  sample_n(8) %>% 
  ungroup() %>% 
  pull(work)

downsampled <- corpus %>% 
  filter(author %in% a & work %in% w)

head(downsampled)
```

Authors with less than 10 works:

```{r}
sm_samples <- corpus %>% 
  count(author, work, sort = T) %>% 
  group_by(author) %>% 
  count(sort = T) %>% 
  filter(n < 10) %>% pull(author)

sm_samples
```

Extract only "small samples" (in terms of n of works) authors, prepare sets with different number of MFW

```{r}
# subset
corpus_s <- corpus %>% 
  filter(author %in% sm_samples) %>% 
  # attach other authors downsampled
  rbind(downsampled)

corpus_s %>% 
  count(work) %>% nrow() # 112 works to test

# number of chunks for each author
corpus_s %>% 
  count(author, sort = T)
```

#### Ranks

```{r}
# total words
total <- corpus_s %>% 
  unnest_tokens(input = text, output = word, token = "words") %>% 
  nrow()

# check ranks for the 200 MFW
ranks <- corpus_s %>% 
  unnest_tokens(input = text, output = word, token = "words") %>% 
  count(word, sort = T) %>% 
  mutate(rel_freq = (n / total) * 100 ) %>% 
  head(200)

head(ranks, 10)
```

#### 200 MFW

Calculate relative frequencies (L1) of 200 MFW in each chunk

```{r}
rfreq <- corpus_s %>% 
  unnest_tokens(input = text, output = word, token = "words") %>% 
  filter(word %in% ranks$word) %>% 
  group_by(tag) %>% 
  count(word, sort = T) %>% 
  ungroup() %>% 
  mutate(rel_freq = n/2000 * 100) %>% 
  select(-n) %>% 
  pivot_wider(names_from = word, values_from = rel_freq, values_fill = 0) %>% 
  arrange(-desc(tag))

rfreq[1:5, 1:5]

dim(rfreq)

# extract metadata back as separate columns
meta_cols <- rfreq %>% 
  select(tag) %>% 
  separate(remove = FALSE, col = tag, into = c("chunk_num", "othr"), sep = "__") %>% 
  separate(col = "othr", into = c("author", "work"), sep = "_") %>% 
  select(work, author, chunk_num, tag)

head(meta_cols)

# attach metadata cols to word freqs
rfreq <- meta_cols %>% 
  left_join(rfreq, by = "tag")

dim(rfreq)

write.csv(rfreq, "03_tests/authors_themselves/to-test_2000_200mfw_rfreq.csv", 
          row.names = F)
```

#### Experiment summary

In the `03_ground_truth.ipynb` each work by each author is taken as a problem (not all works tested, only selected above). For each work the BDI results against the true author are recorded.

#### Viz

Taking the results from the BDI runs and look into the distributions. The plots are stored in a separate folder, with 1 plot = 1 author.

```{r}
# gather all results in csv
fl <- list.files(path = "03_tests/authors_themselves/bdi_res/", full.names = T)

# transform to long format for the plot
new_data <- purrr::map_df(fl, function(x) {
  mydata <- read.csv(x)
  mydata %>% pivot_longer(!X, names_to = "chunk", values_to = "bdi") %>% 
  mutate(group = x)
})

# count number of chunks for plot labels
n_chunks <- new_data %>% 
  mutate(group = str_remove_all(group, 
                                "03_tests/authors_themselves/bdi_res//|\\.csv")) %>% 
  count(group, sort = T) %>% 
  mutate(n = n/1000)

plot_data <- new_data %>% 
  mutate(group = str_remove_all(group, 
                                "03_tests/authors_themselves/bdi_res//|\\.csv"),
         author = str_extract(group, "[^_]*")) %>% 
  left_join(n_chunks, by = "group") %>% 
  mutate(group = paste0(group, " (n chunks: ", n, ")")) 

# calculate means
means <- plot_data %>% 
  group_by(chunk) %>% 
  summarise(mean_chunk = mean(bdi)) %>% 
  ungroup() %>% 
  left_join(plot_data %>% select(author, group, chunk), by = "chunk") %>% 
  distinct()

means

a = NULL

for (i in 1:length(unique(plot_data$author))) {
  a = unique(plot_data$author)[i]
  fh = paste0("03_tests/authors_themselves/plots/", a, ".png")
  
  plot_data %>% 
    filter(author == a) %>% 
    ggplot(aes(x = bdi,  
             # colour = author, 
             # fill = author,
             group = group)) + 
  geom_density(alpha = 0.05, color = "darkgreen", fill = "darkgreen") + 
  
  geom_point(data = means %>% filter(author == a),
             aes(x = mean_chunk, 
                 y = -1), 
             shape = 8, color = "darkgreen") + 
  
  geom_vline(xintercept=0, lty = 2, colour = "black") + 
  facet_wrap(~group, ncol = 1) + 
  theme(legend.position = "None") + 
  labs(title = "Work tested against its author")
  
  ggsave(filename = fh,
       plot = last_plot(),
       width = 8, height = 10, 
       bg = "white", dpi = 300)
}

```

```{r}
# clean the env
rm(list = ls())
```

# FP1

Testing the FP fragment known to be by Diderot

### corpus prep

Load data

```{r}
# taking FP and ed1774-nch as there should be at least two works of the presumable author
fp1 <- list.files(path = "../../data/test_fragments/",
                       pattern = "FP|ed1774_nch", 
                       full.names = T)

fp1

fp <- tibble(
  path = fp1,
  title = fp1,
  text = sapply(path, read_file)
) %>% 
  mutate(title = str_remove_all(title, "^\\.\\./\\.\\./data/test_fragments//|\\.txt"))

str(fp)
```

Cleaning

```{r}
# clean ' and search for &
# remove "appeared|changed|unchanged"

fp_tokens <- fp %>% 
  # replace ' and - with \s
  mutate(text_cln = str_replace_all(text, "'|-|_", " "),
         # replace newlines
         text_cln = str_replace_all(text_cln, "\\r|\\n", "  "),
         # lowercase
         text_cln = sapply(text_cln, tolower)) %>% #str
  # remove raw texts & paths
  select(-text, -path) %>% 
  rename(text = text_cln) %>% 
  
  # tokenisation
  unnest_tokens(input = text, output = word, token = "words") %>% 
  
  # remove working tags
  filter(!word %in% c("appeared", "changed", "unchanged")) %>% 
  # remove digits
  filter(!str_detect(word, "^\\d*$")) 

head(fp_tokens)
```

### 1k chunks

Create 1000 words chunks

```{r}
fp_chunks <- fp_tokens %>% 
  group_by(title) %>% 
  mutate(id = row_number()-1,
         id_group = floor(id /1000)) %>% 
  ungroup()

head(fp_chunks)

# find chunks with less than 1000 words
x <- fp_chunks %>% 
  group_by(title, id_group) %>% 
  count() %>% 
  ungroup() %>% 
  filter(n < 1000) %>% 
  mutate(chunk_id = paste0(title, "__", id_group)) 

x # small chunks

fp_chunks <- fp_chunks %>% 
  mutate(chunk_id = paste0(title, "__", id_group)) %>% 
  # remove shorter chunks
  filter(!chunk_id %in% x$chunk_id) %>% 
  
  # concat text in each chunk
  select(chunk_id, word) %>% 
  group_by(chunk_id) %>% 
  mutate(text = paste(word, collapse = " ")) %>%
  select(-word) %>%
  distinct() %>%
  ungroup() %>% 
  # rename chunks (_ to -)
  mutate(chunk_id = str_replace_all(chunk_id, "_", "-"))

head(fp_chunks)

# count number of chunks
fp_chunks %>% 
  separate(col = chunk_id, into = c("title", "chunk_id"), sep = "--") %>% 
  count(title)

rm(fp, fp1, fp_tokens, x)
```

Transform to BDI format

```{r}
fp1_chunks <- fp_chunks %>% 
  separate(col = chunk_id, into = c("work", "chunk_num"), sep = "--") %>% 
  mutate(author = "HDI") %>% 
  select(work, chunk_num, author, text)

head(fp1_chunks)
```

Prepare reference corpus and merge with FP data

```{r}
corpus <- readRDS("../../data/bdi/corpus_chunks.rds")

raw_corpus <- corpus %>% 
  separate(chunk_id, into = c("chunk_num", "title"), sep = "__") %>% 
  separate(title, into = c("author", "work"), sep = "_") %>% 
  select(author, work, chunk_num, text) %>% 
  # fix Diderot into one set
  # filter(author != "Diderot II") %>% 
  mutate(author = ifelse(author == "Diderot II", "Diderot", author)) %>% 
  # merge
  rbind(fp1_chunks) %>% 
  mutate(tag = paste0(chunk_num, "__", author, "_", work))

head(raw_corpus)

raw_corpus %>% 
  count(author, work, sort = T) 
```

Count word frequencies

```{r}
# total words
total <- raw_corpus %>% 
  unnest_tokens(input = text, output = word, token = "words") %>% 
  nrow()

# check ranks for the 200 MFW
ranks <- raw_corpus %>% 
  unnest_tokens(input = text, output = word, token = "words") %>% 
  count(word, sort = T) %>% 
  mutate(rel_freq = (n / total) * 100 ) %>% 
  head(200) 

head(ranks, 10)

# set number of mfw
mfw100 <- ranks$word[1:100]
mfw200 <- ranks$word[1:200]
```

100 MFW

```{r}
# calculate relative frequencies in each chunk
rfreq <- raw_corpus %>% 
  unnest_tokens(input = text, output = word, token = "words") %>% 
  # filter 100 MFW
  filter(word %in% mfw100) %>% 
  # count words in each chunk
  group_by(tag) %>% 
  count(word, sort = T) %>% 
  ungroup() %>% 
  # calculate relative freq
  mutate(rel_freq = n/1000 * 100) %>% 
  # transform to wide matrix-like table
  select(-n) %>% 
  pivot_wider(names_from = word, values_from = rel_freq, values_fill = 0) %>% 
  arrange(-desc(tag))

rfreq[1:5, 1:5]

dim(rfreq)

#################### save
# extract metadata back as separate columns
meta_cols <- rfreq %>% 
  select(tag) %>% 
  separate(remove = FALSE, col = tag, into = c("chunk_num", "othr"), sep = "__") %>% 
  separate(col = "othr", into = c("author", "work"), sep = "_") %>% 
  select(work, author, chunk_num, tag)

head(meta_cols)

# attach metadata cols to word freqs
rfreq <- meta_cols %>% 
  left_join(rfreq, by = "tag")

dim(rfreq)

write.csv(rfreq, "03_tests/fp1/1000_words/FP1_1000_100mfw_rfreq.csv", 
          row.names = F)
```

200 MFW

```{r}

# calculate relative frequencies in each chunk
rfreq <- raw_corpus %>% 
  unnest_tokens(input = text, output = word, token = "words") %>% 
  # filter 100 MFW
  filter(word %in% mfw200) %>% 
  # count words in each chunk
  group_by(tag) %>% 
  count(word, sort = T) %>% 
  ungroup() %>% 
  # calculate relative freq
  mutate(rel_freq = n/1000 * 100) %>% 
  # transform to wide matrix-like table
  select(-n) %>% 
  pivot_wider(names_from = word, values_from = rel_freq, values_fill = 0) %>% 
  arrange(-desc(tag))

rfreq[1:5, 1:5]

dim(rfreq)

#################### save
# extract metadata back as separate columns
meta_cols <- rfreq %>% 
  select(tag) %>% 
  separate(remove = FALSE, col = tag, into = c("chunk_num", "othr"), sep = "__") %>% 
  separate(col = "othr", into = c("author", "work"), sep = "_") %>% 
  select(work, author, chunk_num, tag)

head(meta_cols)

# attach metadata cols to word freqs
rfreq <- meta_cols %>% 
  left_join(rfreq, by = "tag")

dim(rfreq)

write.csv(rfreq, "03_tests/fp1/1000_words/FP1_1000_200mfw_rfreq.csv", row.names = F)
```

```{r}
# clean env
rm(list = ls())
```

### 2k words chunks

```{r}
# taking FP and ed1774-nch as there should be at least two works of the presumable author
fp1 <- list.files(path = "../../data/test_fragments/",
                       pattern = "FP|ed1774_CH", 
                       full.names = T)

fp1

fp <- tibble(
  path = fp1,
  title = fp1,
  text = sapply(path, read_file)
) %>% 
  mutate(title = str_remove_all(title, "^\\.\\./\\.\\./data/test_fragments//|\\.txt"))

str(fp)

# clean ' and search for &
# remove "appeared|changed|unchanged"

fp_tokens <- fp %>% 
  # replace ' and - with \s
  mutate(text_cln = str_replace_all(text, "'|-|_", " "),
         # replace newlines
         text_cln = str_replace_all(text_cln, "\\r|\\n", "  "),
         # lowercase
         text_cln = sapply(text_cln, tolower)) %>% #str
  # remove raw texts & paths
  select(-text, -path) %>% 
  rename(text = text_cln) %>% 
  
  # tokenisation
  unnest_tokens(input = text, output = word, token = "words") %>% 
  
  # remove working tags
  filter(!word %in% c("appeared", "changed", "unchanged")) %>% 
  # remove digits
  filter(!str_detect(word, "^\\d*$")) 

head(fp_tokens)
```

Chunks

```{r}
fp_chunks <- fp_tokens %>% 
  group_by(title) %>% 
  mutate(id = row_number()-1,
         id_group = floor(id /2000)) %>% 
  ungroup()

head(fp_chunks)

# find chunks with less than 1000 words
x <- fp_chunks %>% 
  group_by(title, id_group) %>% 
  count() %>% 
  ungroup() %>% 
  filter(n < 2000) %>% 
  mutate(chunk_id = paste0(title, "__", id_group)) 

x # small chunks

fp_chunks <- fp_chunks %>% 
  mutate(chunk_id = paste0(title, "__", id_group)) %>% 
  # remove shorter chunks
  filter(!chunk_id %in% x$chunk_id) %>% 
  
  # concat text in each chunk
  select(chunk_id, word) %>% 
  group_by(chunk_id) %>% 
  mutate(text = paste(word, collapse = " ")) %>%
  select(-word) %>%
  distinct() %>%
  ungroup() %>% 
  # rename chunks (_ to -)
  mutate(chunk_id = str_replace_all(chunk_id, "_", "-"))

head(fp_chunks)


rm(fp, fp1, fp_tokens, x)

# count number of chunks
fp_chunks %>% 
  separate(col = chunk_id, into = c("title", "chunk_id"), sep = "--") %>% 
  count(title)
```

Transform to BDI form

```{r}
fp1_chunks <- fp_chunks %>% 
  separate(col = chunk_id, into = c("work", "chunk_num"), sep = "--") %>% 
  mutate(author = "HDI") %>% 
  select(author, work, chunk_num, text) %>% 
  mutate(tag = paste0(chunk_num, "__", author, "_", work))

head(fp1_chunks)
```

Attach main data

```{r}
corpus <- readRDS("../../data/bdi/corpus_chunks.rds")

raw_corpus <- corpus %>% 
  separate(chunk_id, into = c("chunk_num", "title"), sep = "__") %>% 
  separate(title, into = c("author", "work"), sep = "_") %>% 
  select(author, work, chunk_num, text) %>%
  
  # fix Diderot into one set
  mutate(author = ifelse(author == "Diderot II", "Diderot", author)) %>% 
  
  # recalculate chunks to 2k
  mutate(chunk_num_2 = floor(as.numeric(chunk_num)/2)) %>% 
  mutate(tag = paste0(chunk_num_2, "__", author, "_", work)) 

# last 1k chunks (to be removed)
x <- raw_corpus %>% 
  group_by(tag) %>% 
  count(chunk_num_2, sort = T) %>% 
  filter(n < 2) %>% 
  ungroup() %>% 
  pull(tag)

head(x)

raw_corpus <- raw_corpus %>% 
  # remove chunks with less then 2k words
  select(-chunk_num_2) %>% 
  filter(!tag %in% x) %>% 
  
  # merge with fp1
  rbind(fp1_chunks)
  

head(raw_corpus)
```

Count frequencies & write data

```{r}
# total words # 8 868 000
total <- raw_corpus %>% 
  unnest_tokens(input = text, output = word, token = "words") %>%
  nrow()

# check ranks
ranks <- raw_corpus %>% 
  unnest_tokens(input = text, output = word, token = "words") %>% 
  count(word, sort = T) %>% 
  mutate(rel_freq = (n / total) * 100 ) %>% 
  head(200) 

head(ranks, 10)

# set number of mfw
mfw100 <- ranks$word[1:100]
mfw200 <- ranks$word[1:200]
```

100 MFW

```{r}
# calculate relative frequencies in each chunk
rfreq <- raw_corpus %>% 
  unnest_tokens(input = text, output = word, token = "words") %>% 
  # filter MFW
  filter(word %in% mfw100) %>% 
  # count words in each chunk
  group_by(tag) %>% # in this case tag = 2 1000k chunks
  count(word, sort = T) %>% 
  ungroup() %>% 
  # calculate relative freq
  mutate(rel_freq = n/2000 * 100) %>% 
  # transform to wide matrix-like table
  select(-n) %>% 
  pivot_wider(names_from = word, values_from = rel_freq, values_fill = 0) %>% 
  arrange(-desc(tag))

rfreq[1:5, 1:5]

dim(rfreq)

#################### save
# extract metadata back as separate columns
meta_cols <- rfreq %>% 
  select(tag) %>% 
  separate(remove = FALSE, col = tag, into = c("chunk_num", "othr"), sep = "__") %>% 
  separate(col = "othr", into = c("author", "work"), sep = "_") %>% 
  select(work, author, chunk_num, tag)

head(meta_cols)

# attach metadata cols to word freqs
rfreq <- meta_cols %>% 
  left_join(rfreq, by = "tag")

dim(rfreq)

write.csv(rfreq, "03_tests/fp1/2000_words/FP1_2000_100mfw_rfreq.csv", 
          row.names = F)
```

200 MFW

```{r}
# calculate relative frequencies in each chunk
rfreq <- raw_corpus %>% 
  unnest_tokens(input = text, output = word, token = "words") %>% 
  # filter MFW
  filter(word %in% mfw200) %>% 
  # count words in each chunk
  group_by(tag) %>% # in this case tag = 2 1000k chunks
  count(word, sort = T) %>% 
  ungroup() %>% 
  # calculate relative freq
  mutate(rel_freq = n/2000 * 100) %>% 
  # transform to wide matrix-like table
  select(-n) %>% 
  pivot_wider(names_from = word, values_from = rel_freq, values_fill = 0) %>% 
  arrange(-desc(tag))

rfreq[1:5, 1:5]

dim(rfreq)

#################### save
# extract metadata back as separate columns
meta_cols <- rfreq %>% 
  select(tag) %>% 
  separate(remove = FALSE, col = tag, into = c("chunk_num", "othr"), sep = "__") %>% 
  separate(col = "othr", into = c("author", "work"), sep = "_") %>% 
  select(work, author, chunk_num, tag)

head(meta_cols)

# attach metadata cols to word freqs
rfreq <- meta_cols %>% 
  left_join(rfreq, by = "tag")

dim(rfreq)

write.csv(rfreq, "03_tests/fp1/2000_words/FP1_2000_200mfw_rfreq.csv", row.names = F)
```

```{r}
rm(list = ls())
```

## Results

Output from ipynb notebook

### 1k

100 mfw

```{r}
# list csv results
fl <- list.files(path = "03_tests/fp1/1000_words/mfw100/", pattern = ".csv", full.names = T)

new_data <- purrr::map_df(fl, function(x) {
  mydata <- read.csv(x)
  mydata %>% pivot_longer(!X, names_to = "run", values_to = "bdi") %>% 
  mutate(group = x)
})

new_data %>% 
  mutate(group = str_remove_all(group, "03_tests/fp1/1000_words/mfw100//|\\.csv")) %>% 
  ggplot(aes(x = bdi, group = group, colour = group, fill = group)) + 
  geom_density(alpha = 0.5) + 
  geom_vline(xintercept=0, lty = 2, colour = "black") + 
  #geom_vline(xintercept=0.1, lty = 2, colour = "black") + 
  facet_wrap(~group, ncol=1) + 
  theme(legend.position = "None") + 
  labs(title = "FP1 vs various authors: 1000 chunks, 100 MFW")

ggsave(filename = "03_tests/fp1/1000_words/fp1_1k_100mfw_all.png", 
       plot = last_plot(),
       height = 20, width = 8, bg = "white")

new_data %>% 
  mutate(group = str_remove_all(group, "03_tests/fp1/1000_words/mfw100//|\\.csv")) %>% 
  filter(str_detect(group, "Baudeau|dHolbach|Diderot|Raynal")) %>% 
  ggplot(aes(x = bdi, group = run, colour = group, fill = group)) + 
  geom_density(alpha = 0.5) + 
  geom_vline(xintercept=0, lty = 2, colour = "black") + 
  #geom_vline(xintercept=0.1, lty = 2, colour = "black") + 
  facet_wrap(~group, ncol=1) + 
  theme(legend.position = "None") + 
  labs(title = "FP1 vs various authors")
```

# % above 0

```{r}
x <- new_data %>% 
  mutate(group = str_remove_all(group, "03_tests/fp1/2000_words/mfw200//|\\.csv"))#%>% 
 # filter(str_detect(group, "Baudeau|dHolbach|Diderot|Raynal")) 

glimpse(x)



# if we have a vec of 8 values, 4 above 0, and 4 below
v <- c(1, 2, 0.4994, 0.49, -1, -0.24, -0.11, -0.2)

length(v)

sum((v) > 0)/length(v)*100


# 
# x %>% 
#   group_by(run, group) %>% 
#   

# total in each group = 1000
x %>% 
  group_by(group, run) %>% 
  count()

# calculate value above 0 for each run of each group
x %>% 
  # filter(group == "fp1_vs_Baudeau") %>% 
  mutate(above_0 = bdi>0) %>%  
  group_by(group, run) %>% 
  summarise(perc_above_0 = (sum(above_0) / 1000) * 100) 

# summarise by mean % above 0 for each author
x %>% 
  # filter(group == "fp1_vs_Baudeau") %>% 
  mutate(above_0 = bdi>0) %>%  
  group_by(group, run) %>% 
  summarise(perc_above_0 = (sum(above_0) / 1000) * 100) %>% 
  ungroup() %>% 
  group_by(group) %>% 
  summarise(mean_above_0 = mean(perc_above_0)) %>% 
  # arrange by top mean
  arrange(desc(mean_above_0))
```

200 mfw

```{r}
# list csv results
fl <- list.files(path = "03_tests/fp1/1000_words/mfw200/", pattern = ".csv", full.names = T)

new_data <- purrr::map_df(fl, function(x) {
  mydata <- read.csv(x)
  mydata %>% pivot_longer(!X, names_to = "run", values_to = "bdi") %>% 
  mutate(group = x)
})

new_data %>% 
  mutate(group = str_remove_all(group, "03_tests/fp1/1000_words/mfw200//|\\.csv")) %>% 
  ggplot(aes(x = bdi, group = group, colour = group, fill = group)) + 
  geom_density(alpha = 0.5) + 
  geom_vline(xintercept=0, lty = 2, colour = "black") + 
  #geom_vline(xintercept=0.1, lty = 2, colour = "black") + 
  facet_wrap(~group, ncol=1) + 
  theme(legend.position = "None") + 
  labs(title = "FP1 vs various authors: 1000 chunks, 200 MFW")

ggsave(filename = "03_tests/fp1/1000_words/fp1_1k_200mfw_all.png", 
       plot = last_plot(),
       height = 20, width = 8, bg = "white")

new_data %>% 
  mutate(group = str_remove_all(group, "03_tests/fp1/1000_words/mfw200//|\\.csv")) %>% 
  filter(str_detect(group, "Baudeau|dHolbach|Diderot|Raynal")) %>% 
  ggplot(aes(x = bdi, group = run, colour = group, fill = group)) + 
  geom_density(alpha = 0.5) + 
  geom_vline(xintercept=0, lty = 2, colour = "black") + 
  #geom_vline(xintercept=0.1, lty = 2, colour = "black") + 
  facet_wrap(~group, ncol=1) + 
  theme(legend.position = "None") + 
  labs(title = "FP1 vs various authors")
```

### 2k

NB only 3 chunks

100 mfw

```{r}
# list csv results
fl <- list.files(path = "03_tests/fp1/2000_words/mfw100/", pattern = ".csv", full.names = T)

new_data <- purrr::map_df(fl, function(x) {
  mydata <- read.csv(x)
  mydata %>% pivot_longer(!X, names_to = "run", values_to = "bdi") %>% 
  mutate(group = x)
})

new_data %>% 
  mutate(group = str_remove_all(group, "03_tests/fp1/2000_words/mfw100//|\\.csv")) %>% 
  ggplot(aes(x = bdi, group = group, colour = group, fill = group)) + 
  geom_density(alpha = 0.5) + 
  geom_vline(xintercept=0, lty = 2, colour = "black") + 
  #geom_vline(xintercept=0.1, lty = 2, colour = "black") + 
  facet_wrap(~group, ncol=1) + 
  theme(legend.position = "None") + 
  labs(title = "FP1 vs various authors (2k chunks, 100 MFW)")

ggsave(filename = "03_tests/fp1/2000_words/fp1_2k_100MFW.png", plot = last_plot(),
       height = 20, width = 8, bg = "white")


new_data %>% 
  mutate(group = str_remove_all(group, "03_tests/fp1/2000_words/mfw100//|\\.csv")) %>% 
  filter(str_detect(group, "Diderot|dHolbach|Condorcet|Deleyre|Raynal")) %>% 
  mutate(gr = ifelse(str_detect(group, "Diderot"), "d", "no")) %>% 
  ggplot(aes(x = bdi, group = run, colour = gr, fill = gr)) + 
  geom_density(alpha = 0.5) + 
  geom_vline(xintercept=0, lty = 2, colour = "black") + 
  
  scale_color_manual(values = met.brewer("Kandinsky")[2:4]) + 
  scale_fill_manual(values = met.brewer("Kandinsky")[2:4]) + 
  #geom_vline(xintercept=0.1, lty = 2, colour = "black") + 
  facet_wrap(~group, ncol=1) + 
  theme(legend.position = "None") + 
  labs(title = "FP vs selected authors",
       y = "") + 
  theme(strip.text = element_text(size = 10))

# ggsave("../../hdi_diderot/scr/plots_paper/fig_1-c.png",
#        plot = last_plot(), dpi = 300, bg = "white",
#        height = 5, width = 6)
```

200 mfw

```{r}

# list csv results
fl <- list.files(path = "03_tests/fp1/2000_words/mfw200/", pattern = ".csv", full.names = T)

new_data <- purrr::map_df(fl, function(x) {
  mydata <- read.csv(x)
  mydata %>% pivot_longer(!X, names_to = "run", values_to = "bdi") %>% 
  mutate(group = x)
})

new_data %>% 
  mutate(group = str_remove_all(group, "03_tests/fp1/2000_words/mfw200//|\\.csv")) %>% 
  ggplot(aes(x = bdi, group = group, colour = group, fill = group)) + 
  geom_density(alpha = 0.5) + 
  geom_vline(xintercept=0, lty = 2, colour = "black") + 
  #geom_vline(xintercept=0.1, lty = 2, colour = "black") + 
  facet_wrap(~group, ncol=1) + 
  theme(legend.position = "None") + 
  labs(title = "FP1 vs various authors (2k chunks, 200 MFW)")

ggsave(filename = "03_tests/fp1/2000_words/fp1_2k_200MFW.png", plot = last_plot(),
       height = 20, width = 8, bg = "white")


fig_1c <- new_data %>% 
  mutate(group = str_remove_all(group, "03_tests/fp1/2000_words/mfw200//|\\.csv")) %>% 
  filter(str_detect(group, "Diderot|dHolbach|Condorcet|Deleyre|Raynal")) %>% 
  mutate(gr = ifelse(str_detect(group, "Diderot"), "d", "no")) %>% 
  mutate(group = ifelse(group == "fp1_vs_dHolbach", "fp1_vs_d'Holbach", group)) %>% 
  ggplot(aes(x = bdi, group = run, colour = gr, fill = gr)) + 
  geom_density(alpha = 0.5) + 
  geom_vline(xintercept=0, lty = 2, colour = "black") + 
  
  scale_color_manual(values = met.brewer("Kandinsky")[2:4]) + 
  scale_fill_manual(values = met.brewer("Kandinsky")[2:4]) + 
  #geom_vline(xintercept=0.1, lty = 2, colour = "black") + 
  facet_wrap(~group, ncol=1) + 
  theme(legend.position = "None") + 
  labs(title = "FP vs selected authors (2000 words chunks, 200 MFW)",
       y = "") + 
  theme(strip.text = element_text(size = 12))

fig_1c

ggsave("../plots_paper/fig_1-c.png",
       plot = fig_1c, dpi = 300, bg = "white",
       height = 5, width = 6)

rm(list = ls())
```

### Fig 1b+1c

```{r}
# fig 1b should exist!

plot_grid(fig_1b, fig_1c, 
          ncol = 2,
          labels = c('B', 'C'),
          label_size = 24,
          rel_widths = c(2, 2))

ggsave("../plots_paper/fig_1-bc.png",
       bg = "white", dpi = 300,
       height = 6, width = 10)
```

# Known work vs all

This part of the notebook test the method that will be used for the analysis on the known works of each author. Two works are taken for each author, then one is tested against the whole corpus, seeing which author will be selected by BDI as the closest.

### data preparation

Load the corpus

```{r}
corpus <- readRDS("../../data/bdi/corpus_chunks.rds")
glimpse(corpus)
```

Transform to 2000-word chunks

```{r}
corpus <- corpus %>% 
  separate(chunk_id, into = c("chunk_num", "title"), sep = "__") %>% 
  separate(title, into = c("author", "work"), sep = "_") %>% 
  select(author, work, chunk_num, text) %>% 
  # fix Diderot into one set
  mutate(author = ifelse(author == "Diderot II", "Diderot", author)) %>% 
  
  # recalculate chunks
  mutate(chunk_num_2 = floor(as.numeric(chunk_num)/2)) %>% 
  mutate(tag = paste0(chunk_num_2, "__", author, "_", work)) 

# last 1k chunks (to be removed)
x <- corpus %>% 
  group_by(tag) %>% 
  count(chunk_num_2, sort = T) %>% 
  filter(n < 2) %>% 
  ungroup() %>% 
  pull(tag)

head(x)

raw_corpus <- corpus %>% 
  # remove chunks with less then 2k words
  select(-chunk_num_2) %>% 
  filter(!tag %in% x) %>% 
  mutate(chunk_num = str_extract(tag, "^\\d+"))
  
head(raw_corpus) # in this corpus each 2k chunk = 2 rows

rm(x)
```

### downsampling

Select only two works by each author

```{r}
works_s <- raw_corpus %>% 
  group_by(author, work) %>% 
  count(sort = T) %>% 
  ungroup() %>%
  group_by(author) %>% 
  filter(n > 5 & n < 100) %>% 
  sample_n(2) %>% 
  ungroup() %>% 
  select(-n) %>% 
  mutate(tag_s = paste0(author, "__", work))

works_s
```

Extract selected works

```{r}
corpus_s <- raw_corpus %>% 
  mutate(tag = paste0(author, "__", work)) %>% 
  filter(tag %in% works_s$tag_s) 

# count number of chunks in selected works
corpus_s %>% 
  group_by(author, work) %>% 
  count() %>% 
  mutate(n = n/2) # 2000-word chunks
```

Downsample number of chunks to 20 or less

look NB !!

```{r}

glimpse(corpus_s)

to_cut <- corpus_s %>% 
  count(tag) %>% 
  filter(n > 20) %>% # NB these are only 10 chunks of 2000 words !!!
  pull(tag)

# select randomply 20 chunks
cutted <- corpus_s %>% 
  filter(tag %in% to_cut) %>% 
  group_by(tag) %>% 
  sample_n(20) 

# merge
corpus_s <- corpus_s %>% 
  # select only small works
  filter(!tag %in% to_cut) %>% 
  rbind(cutted) 
```

### count ranks & freqs

```{r}
# total words 
total <- corpus_s %>% 
  unnest_tokens(input = text, output = word, token = "words") %>% 
  nrow()

# check ranks for the 200 MFW
ranks <- corpus_s %>% 
  unnest_tokens(input = text, output = word, token = "words") %>% 
  count(word, sort = T) %>% 
  mutate(rel_freq = (n / total) * 100 ) %>% 
  head(200)

head(ranks, 10)

# set MFW number
mfw <- ranks$word[1:200]

# calculate relative frequencies in each chunk
rfreq <- corpus_s %>% 
  mutate(tag = paste0(chunk_num, "__", author, "_", work)) %>% 
  # select(author, work, chunk_num, text, tag) %>% 
  unnest_tokens(input = text, output = word, token = "words") %>% 
  # filter 100 MFW
  filter(word %in% mfw) %>% 
  # count words in each chunk
  group_by(tag) %>% 
  count(word, sort = T) %>% 
  ungroup() %>% 
  # calculate relative freq
  mutate(rel_freq = n/2000 * 100) %>% 
  # transform to wide matrix-like table
  select(-n) %>% 
  pivot_wider(names_from = word, values_from = rel_freq, values_fill = 0) %>% 
  arrange(-desc(tag))

rfreq[1:5, 1:5]

dim(rfreq) 

#################### save
# extract metadata back as separate columns (from tag-merged freqs already!)
meta_cols <- rfreq %>% 
  select(tag) %>% 
  separate(remove = FALSE, 
           col = tag, into = c("chunk_num", "othr"), sep = "__") %>% 
  separate(col = "othr", into = c("author", "work"), sep = "_") %>% 
  select(work, author, chunk_num, tag)

head(meta_cols)

# attach metadata cols to word freqs
rfreq <- meta_cols %>% 
  left_join(rfreq, by = "tag")

dim(rfreq)

write.csv(rfreq, "03_tests/authors_vs_all/all_2works_2k_200mfw_rfreq.csv", 
          row.names = F)
```

### Viz

```{r}
all_res <- list.files(path = "03_tests/authors_vs_all/runs/",
                      full.names = TRUE)

x <- tibble(path = all_res)

meta_res <- x %>% 
  mutate(path_cln = str_remove_all(path, 
                                   "03_tests/authors_vs_all/runs//|\\.csv"),
         true_author = str_extract(path_cln, "^(\\w+)"),
         work = str_extract(path_cln, "-.*?_vs"),
         work = str_remove_all(work, "-|_vs")
         ) 

meta_res %>% 
  group_by(true_author, work) %>% 
  count()
```

The loop below takes each author results and make a plot for them, highlighting the true author

```{r, warning=FALSE, message=FALSE}
glimpse(meta_res)

unique(meta_res$true_author)

for (i in 1:length(unique(meta_res$true_author))) {
  
  a <- unique(meta_res$true_author)[i]

  # select one author
  x <- meta_res %>% 
    filter(true_author == a)
  
  # read the csv file and transform to a long table
  fl <- x %>% pull(path)
  
  new_data <- purrr::map_df(fl, function(x) {
    mydata <- read.csv(x)
    mydata %>% pivot_longer(!X, names_to = "run", values_to = "bdi") %>% 
      mutate(group = x)
  })
  
  true_author_pattern <- paste0("_vs_", a)
  
  new_data <- new_data %>% 
      mutate(group = str_remove_all(group, 
                                    "03_tests/authors_vs_all/runs//|\\.csv"),
             author = ifelse(str_detect(group, true_author_pattern), 
                             "true_author", "else"),
             work = str_extract(group, "-.*?_vs"),
             work = str_remove_all(work, "-|_vs")) 
  
  p1 <- new_data %>% 
    filter(work == new_data$work[1]) %>% 
    
      ggplot(aes(x = bdi, group = run)) + 
      geom_density(alpha = 0.15, colour = "black", aes(fill = author)) + 
      geom_vline(xintercept=0, lty = 2, colour = "black") + 
      facet_wrap(~group, ncol=1) +
      theme(legend.position = "None") +
      scale_fill_manual(values = c(met.brewer("Kandinsky")[3], # yellow
                                   met.brewer("Kandinsky")[2])) +
      labs(title = paste0(a, " vs various authors (2k chunks, 200 MFW)"),
           subtitle = "Work 1")
  
  p2 <- new_data %>% 
    filter(work == new_data$work[2]) %>% 
    
      ggplot(aes(x = bdi, group = run)) + 
      geom_density(alpha = 0.15, colour = "black", aes(fill = author)) + 
      geom_vline(xintercept=0, lty = 2, colour = "black") + 
      facet_wrap(~group, ncol=1) +
      theme(legend.position = "None") +
      scale_fill_manual(values = c(met.brewer("Kandinsky")[3], # yellow
                                   met.brewer("Kandinsky")[2])) + 
      labs(title = "",
           subtitle = "Work 2")
  
  plot_grid(p1, p2)
  
  fh <- paste0("03_tests/authors_vs_all/plots/", a, ".png")
  
  ggsave(filename = fh,
       plot = last_plot(), bg = "white", width = 10, height = 20)
  
}

```

```{r}
rm(list = ls())
```

## Particular authors exploration

### data preparation

Here only selected authors with more works are being analysed.

```{r}
corpus <- readRDS("../../data/bdi/corpus_chunks.rds")
glimpse(corpus)
```

```{r}
corpus <- corpus %>% 
  separate(chunk_id, into = c("chunk_num", "title"), sep = "__") %>% 
  separate(title, into = c("author", "work"), sep = "_") %>% 
  select(author, work, chunk_num, text) %>% 
  # fix Diderot into one set
  mutate(author = ifelse(author == "Diderot II", "Diderot", author)) %>% 
  
  # recalculate chunks
  mutate(chunk_num_2 = floor(as.numeric(chunk_num)/2)) %>% 
  mutate(tag = paste0(chunk_num_2, "__", author, "_", work)) 

# last 1k chunks (to be removed)
x <- corpus %>% 
  group_by(tag) %>% 
  count(chunk_num_2, sort = T) %>% 
  filter(n < 2) %>% 
  ungroup() %>% 
  pull(tag)

head(x)

raw_corpus <- corpus %>% 
  # remove chunks with less then 2k words
  select(-chunk_num_2) %>% 
  filter(!tag %in% x) %>% 
  mutate(chunk_num = str_extract(tag, "^\\d+"))
  
head(raw_corpus) # in this corpus each 2k chunk = 2 rows

rm(x)
```

### downsampling

Select 5 works of 5 authors

```{r}
#unique(raw_corpus$author)
authors <- c("Diderot", "dHolbach", "Baudeau", "Condorcet", "Raynal")

works_s <- raw_corpus %>% 
  # filter authors
  filter(author %in% authors) %>% 
  group_by(author, work) %>% 
  count(sort = T) %>% 
  ungroup() %>%
  
  # filter works
  group_by(author) %>% 
  filter(n > 5 & n < 100) %>% 
  sample_n(5) %>% 
  ungroup() %>% 
  select(-n) %>% 
  mutate(tag_s = paste0(author, "__", work))

works_s
```

Extract data & downsample works which are too long

```{r}
corpus_s <- raw_corpus %>% 
  mutate(tag = paste0(author, "__", work)) %>% 
  filter(tag %in% works_s$tag_s) 

# count number of chunks in selected works
corpus_s %>% 
  group_by(author, work) %>% 
  count() %>% 
  mutate(n = n/2) # 2000-word chunks


# downsample works with more than 20 2000-word chunks
to_cut <- corpus_s %>% 
  count(tag) %>% 
  filter(n > 40) %>% 
  pull(tag)

# select randomply 20 chunks
cutted <- corpus_s %>% 
  filter(tag %in% to_cut) %>% 
  group_by(tag) %>% 
  sample_n(40) 

# merge
corpus_s <- corpus_s %>% 
  # select only small works
  filter(!tag %in% to_cut) %>% 
  rbind(cutted) 
```

### count word freqs & write data

```{r}
# total words 
total <- corpus_s %>% 
  unnest_tokens(input = text, output = word, token = "words") %>% 
  nrow()

# check ranks for the 200 MFW
ranks <- corpus_s %>% 
  unnest_tokens(input = text, output = word, token = "words") %>% 
  count(word, sort = T) %>% 
  mutate(rel_freq = (n / total) * 100 ) %>% 
  head(200)

head(ranks, 10)

# set MFW number
mfw <- ranks$word[1:200]

# calculate relative frequencies in each chunk
rfreq <- corpus_s %>% 
  mutate(tag = paste0(chunk_num, "__", author, "_", work)) %>% 
  # select(author, work, chunk_num, text, tag) %>% 
  unnest_tokens(input = text, output = word, token = "words") %>% 
  # filter 100 MFW
  filter(word %in% mfw) %>% 
  # count words in each chunk
  group_by(tag) %>% 
  count(word, sort = T) %>% 
  ungroup() %>% 
  # calculate relative freq
  mutate(rel_freq = n/2000 * 100) %>% 
  # transform to wide matrix-like table
  select(-n) %>% 
  pivot_wider(names_from = word, values_from = rel_freq, values_fill = 0) %>% 
  arrange(-desc(tag))

rfreq[1:5, 1:5]

dim(rfreq) 

#################### save
# extract metadata back as separate columns (from tag-merged freqs already!)
meta_cols <- rfreq %>% 
  select(tag) %>% 
  separate(remove = FALSE, 
           col = tag, into = c("chunk_num", "othr"), sep = "__") %>% 
  separate(col = "othr", into = c("author", "work"), sep = "_") %>% 
  select(work, author, chunk_num, tag)

head(meta_cols)

# attach metadata cols to word freqs
rfreq <- meta_cols %>% 
  left_join(rfreq, by = "tag")

dim(rfreq)

write.csv(rfreq, "03_tests/authors_vs_all/5-authors_5works_2k_200mfw_rfreq.csv", 
          row.names = F)
```

### viz

```{r}
all_res <- list.files(path = "03_tests/authors_vs_all/runs_5-authors/",
                      full.names = TRUE)

x <- tibble(path = all_res)

meta_res <- x %>% 
  mutate(path_cln = str_remove_all(path, 
                                   "03_tests/authors_vs_all/runs_5-authors//|\\.csv"),
         true_author = str_extract(path_cln, "^(\\w+)"),
         work = str_extract(path_cln, "-.*?_vs"),
         work = str_remove_all(work, "-|_vs")
         ) 

meta_res %>% 
  group_by(true_author, work) %>% 
  count()
```

Quick fn for plots

```{r}
plt <- function(df, n, a) {
  p <- df %>% 
    filter(work == unique(df$work)[n]) %>% 
    
      ggplot(aes(x = bdi, group = run)) + 
      geom_density(alpha = 0.15, colour = "black", aes(fill = author)) + 
      geom_vline(xintercept=0, lty = 2, colour = "black") + 
      facet_wrap(~group, ncol=1) +
      theme(legend.position = "None") +
      scale_fill_manual(values = c(met.brewer("Kandinsky")[3], # yellow
                                   met.brewer("Kandinsky")[2])) +
      labs(#title = paste0(a, " vs various authors (2k chunks, 200 MFW)"),
           subtitle = paste0("Work ", n))
}

# p1 <- plt(new_data, 1, a)
# p1
```

```{r, warning=FALSE, message=FALSE}
#glimpse(meta_res)

unique(meta_res$true_author)

for (i in 1:length(unique(meta_res$true_author))) {
  
  a <- unique(meta_res$true_author)[i]

  # select one author
  x <- meta_res %>% 
    filter(true_author == a)
  
  # read the csv file and transform to a long table
  fl <- x %>% pull(path)
  
  new_data <- purrr::map_df(fl, function(x) {
    mydata <- read.csv(x)
    mydata %>% pivot_longer(!X, names_to = "run", values_to = "bdi") %>% 
      mutate(group = x)
  })
  
  true_author_pattern <- paste0("_vs_", a)
  
  new_data <- new_data %>% 
      mutate(group = str_remove_all(group, 
                                    "03_tests/authors_vs_all/runs_5-authors//|\\.csv"),
             author = ifelse(str_detect(group, true_author_pattern), 
                             "true_author", "else"),
             work = str_extract(group, "-.*?_vs"),
             work = str_remove_all(work, "-|_vs")) 
  
  
  p1 <- plt(new_data, n = 1, a)
  p2 <- plt(new_data, n = 2, a)
  p3 <- plt(new_data, n = 3, a)
  p4 <- plt(new_data, n = 4, a)
  p5 <- plt(new_data, n = 5, a)
  
  plot_grid(p1, p2, p3, p4, p5, nrow = 1)
  
  fh <- paste0("03_tests/authors_vs_all/plots_5-authors/", a, ".png")
  
  ggsave(filename = fh,
       plot = last_plot(), bg = "white", width = 15, height = 7)
  
}

```
